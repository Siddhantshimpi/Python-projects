{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, matplotlib.pyplot as plt, PIL\n",
    "from IPython.display import Image\n",
    "\n",
    "plt.style.use(['science', 'notebook', 'grid', 'dark_background'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Initial Problem:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a dataset $\\vec p=[1,3,5,2...]$ of length $N$. We wanna know the corresponding matching dataset $\\vec q=[q_1,q_2,q_3..]$ is such that this Loss Function $$H(\\vec p,\\vec q)=-\\sum_{i=1}^Np_i\\,\\ln(q_i)$$  is minimized with the constraint $\\sum_i p_i=\\sum_i q_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-22.923775636027425"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([5, 1, 4, 6, 2, 4])\n",
    "q1 = p\n",
    "\n",
    "q2 = np.array([3, 7, 1, 4, 1, 6])\n",
    "q3 = np.array([2, 5, 7, 2, 1, 5])\n",
    "\n",
    "-sum(p * np.log(q2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the set of numbers that minimizes $H$ is exactly $q=p$. We can prove this.\n",
    "\n",
    "We wanna maximize a function with respect to a constriant, so this is a Lagrange Multiplier problem. We find the stationary points of $$f(\\vec q,\\lambda)\\equiv -\\sum_{i=1}^Np_i\\,\\ln(q_i)=-\\lambda\\Bigg(\\sum_i p_i-\\sum_i q_i\\Bigg)$$\n",
    "\n",
    "We set the first derivative with respect to each of $q_i$ equal to zero, and also that respect to $\\lambda$: $$\\frac{\\partial f}{\\partial q_i}=-\\frac{p_i}{q_i}+\\lambda \\to 0$$ and $$\\frac{\\partial f}{\\partial \\lambda}=\\Bigg(\\sum_i p_i-\\sum_i q_i\\Bigg)\\to 0$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we get\n",
    "- $p_i=\\lambda q_i \\implies \\sum_ip_i=\\lambda \\sum_i q_i$\n",
    "- $\\sum_ip_i=\\sum_iq_i$\n",
    "\n",
    "These both can only be true if $\\lambda=1$ so therefore we must have $q_i=p_i$ and thus $\\vec q=\\vec p$.\n",
    "\n",
    "This also works for functions as well. In this case, rather than having $\\vec q$ and $\\vec p$ where $\\sum_i q_i=\\sum_i q_i$, we'd have $\\int_{\\mathbb R}p(x)\\,dx=\\int_{\\mathbb R}q(x)\\,dx$. The function $q(x)$ that minimizes $$\\int_{\\mathbb R}p(x)\\,\\ln(q(x))\\,dx$$ is precisely $q(x)=p(x)$. To prove this, one needs calculus of variation. This is why this function represents 'cross' entropy as it corresponds to a way to measure the difference of a measured function $q$ with respect to a known function $p$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Classification problems, an input (usually an image) is taken in, which we'll call $x$. This image may for example, contain 1 of 5 different objects. We'll call the true likelihood of an image $x$ belonging to a class $i$ as $p_i$. The goal of a Classifier is to create a function $f$ such that $$f(x)=\\vec q$$ where $\\vec q$ is as close to $\\vec p$ as possible.\n",
    "\n",
    "- $\\vec p$ and $\\vec q$ are Probability Mass Functions, and each element of the vector represents a different class. It follows that $\\sum_i p_i=\\sum_iq_i=1$, the constraint that we had in the Lagrange Multiplier problem above.\n",
    "- Typically, we know what class $\\tilde C$ an image $x$ belongs to. In this case, its typically the case that $p_{\\tilde C}=1$ for the class $i=\\tilde C$, that we know it is, and all the other $p_i=0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the difference between $\\vec p$ and $\\vec q$ we can minimize the Loss Function $$H(p,q)=-\\sum_ip_i\\,\\ln(q_i)$$ Since the minima of this function occurs exactly when $q_i=p_i \\,\\forall\\, i$, we know which class an image belongs to: $$H(p,q)=-\\ln(q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " array([0.08389358, 0.13963495, 0.15329469, 0.16223148, 0.05212778,\n",
       "        0.04602886, 0.14695855, 0.00457714, 0.19063502, 0.02061793]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.zeros(10, dtype = int); p[4] = 1\n",
    "q = np.random.rand(10)\n",
    "\n",
    "q = q / sum(q)\n",
    "p, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.95405732])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = -np.log(q[p > 0])\n",
    "H"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we'll see what happens when we make the probability higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0463048])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[4] = 20\n",
    "q = q / sum(q)\n",
    "\n",
    "H = -np.log(q[p > 0])\n",
    "H"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Multiple Images:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, consider we compute this Loss over $N$ images so that the $N$ th one is $x_N$. Suppose also that we know exactly which class the image belongs to. We can thus express the true class of the $n$ th image as $\\tilde C(n)$. We'll express the probability of an image $n$ belonging to a class $\\tilde C$ as $q_n(\\tilde C)$.\n",
    "\n",
    "Thus the predicted probability of the image $x_n$ belonging to its true class $\\tilde C(n)$ is $q_n(\\tilde C(n))$ and we sum it together: $$L(p,q)\\equiv\\sum_{n=1}^NH(p_n,q_n)=-\\sum_{n=1}^N\\ln(q_n(\\tilde C(n)))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.zeros((4, 10), dtype = int)\n",
    "\n",
    "p[0][4] = 1\n",
    "p[1][2] = 1\n",
    "p[2][8] = 1\n",
    "p[3][6] = 1\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17182178, 0.17354272, 0.00264357, 0.11538858, 0.0952921 ,\n",
       "        0.03923513, 0.13330965, 0.02834258, 0.15330713, 0.08711677],\n",
       "       [0.15093522, 0.11082929, 0.00041901, 0.18078421, 0.13535436,\n",
       "        0.0160621 , 0.06940709, 0.02145562, 0.15551819, 0.15923491],\n",
       "       [0.10116303, 0.00282392, 0.01644199, 0.02477249, 0.14041257,\n",
       "        0.08760147, 0.13662187, 0.20654316, 0.09503555, 0.18858395],\n",
       "       [0.09795096, 0.17183749, 0.05300985, 0.11756661, 0.18161321,\n",
       "        0.09791736, 0.07609876, 0.04298613, 0.00126308, 0.15975655]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.random.rand(40).reshape(4, 10)\n",
    "q = q / np.expand_dims(np.sum(q, axis = 1), axis = 1)\n",
    "\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.05766254209766"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_values = -np.log(q[p > 0])\n",
    "L = sum(H_values)\n",
    "\n",
    "L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec q$ should be related to a Probability Density Function where\n",
    "\n",
    "1. Bounded between 0 and 1,\n",
    "2. The closer we get to 0, the less likely we're confident that image $n$ is in class $C$,\n",
    "3. The closer we get to 1, the more likely we're confident that image $n$ is in class $C$, and\n",
    "4. $\\sum_{i=0}^Cq_C=1$ for each image.\n",
    "\n",
    "Suppose a Neural Network outputs $f(x_n)=\\hat y_n$ where $\\hat y_n$ is a vector with the same length as the number of classes, but it's not normalized like $\\vec q$ should be. We can enforce the last condition by normalizing the following way: $$q_n(C)=\\frac{\\exp(\\hat y_n(C))}{\\sum_{C'=0}^C\\exp(y_n(C'))}$$ So we can write our Loss $$L(\\hat y_n)=-\\sum_{n=0}^N\\ln(q_n(\\tilde C(n)))=-\\sum_{n=0}^N\\ln\\Bigg[\\frac{\\exp(\\hat y_n(\\tilde C(n)))}{\\sum_{C'=0}^C\\exp(\\hat y_n(C'))}\\Bigg]$$ \n",
    "\n",
    "For a given $n$, $\\hat y_n(C)$ is the Network Output for the image $x_n$. It's a vector of length 10 (the network outputs 10 numbers per image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.4587013 ,  2.9724858 , 18.21290946,  5.91360428, 15.56538332,\n",
       "        13.31881314,  4.05287638,  0.32273811,  3.75215932, 14.61206258],\n",
       "       [ 4.32240779,  5.33064917,  0.76209144,  7.52115796, 12.07570169,\n",
       "         8.26399902,  8.21536583,  1.33610281,  0.02038071,  1.42684448],\n",
       "       [ 0.28825382, 16.34950972,  0.07133716,  4.03311009,  3.18937161,\n",
       "         5.1043221 ,  1.36384376, 13.16850029,  0.02771731,  0.39320832],\n",
       "       [12.20144929, 14.73635297,  1.82828837,  4.25762684, 15.4446284 ,\n",
       "         0.89874838,  4.82702672,  1.08992772, 10.04904889,  2.80068946]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = 20 * np.random.rand(40).reshape(4, 10) ** 2\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.exp(yhat)\n",
    "q = q / np.expand_dims(np.sum(q, axis = 1), axis = 1)\n",
    "\n",
    "q.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3], dtype=int64), array([4, 2, 8, 6], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_squig = np.where(p)\n",
    "C_squig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.87928504432753"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_values = -np.log(q[C_squig])\n",
    "L = sum(H_values)\n",
    "\n",
    "L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(41.8793, dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
    "L(torch.tensor(yhat), torch.tensor(p, dtype = torch.float))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e86d9026de2d19bb55d307aab591188e1f7cd5b8892ed03c4ba33ce5d5a31e86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
